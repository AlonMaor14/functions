{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Based Sentiment Analysis Model Server\n",
    "The model used here, was trained with the concept of transfer learning  i.e. taking huggingface transformers pretrained BERT model and further training it on a custom dataset of reviews. this yields a sentiment analysis model based on the prior knowledge of BERT. \n",
    "The model server is given a list of texts and outputs a list of labels corresponding to its prediction.\n",
    "The labels express the sentiment of the writer towards the topic of the text:\n",
    "0 for negative sentiment, 1 for neutral and 2 for positive.\n",
    "\n",
    "The model file (~430 MB), can be downloaded to your local environment from: https://iguazio-sample-data.s3.amazonaws.com/models/model.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "from mlrun import mlconf\n",
    "from bert_sentiment_analysis_serving import (\n",
    "    BertSentimentClassifier,\n",
    "    SentimentClassifierServing,\n",
    ")\n",
    "import os\n",
    "\n",
    "mlconf.dbpath = mlconf.dbpath or \"http://mlrun-api:8080\"\n",
    "mlconf.artifact_path = mlconf.artifact_path or f'{os.environ[\"HOME\"]}/artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples locally\n",
    "You may change model_dir to point at the path where model.pt file is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/User/demo_stocks/artifacts/models/bert_sentiment_analysis_model.pt\"\n",
    "model_server = SentimentClassifierServing(\"model-server\", model_dir=model_dir)\n",
    "model_server.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 1\n",
    "Here we test a pretty straightforward example for positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_server.predict(\n",
    "    {\n",
    "        \"instances\": [\n",
    "            \"I had a pleasure to work with such dedicated team. Looking forward to \\\n",
    "             cooperate with each and every one of them again.\"\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "assert output[0] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### example 2\n",
    "Now we will test a couple more examples. These are arguably harder due to misleading words that express, on their own, an opposite sentiment comparing to the full text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_server.predict(\n",
    "    {\n",
    "        \"instances\": [\n",
    "            \"This app is amazingly useless.\",\n",
    "            \"As much as I hate to admit it, the new added feature is surprisingly user friendly.\",\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "assert output[0] == 0\n",
    "assert output[1] == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote activation\n",
    "Create a function object with custom specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlrun import code_to_function, mount_v3io\n",
    "import requests\n",
    "import yaml\n",
    "\n",
    "with open(\"item.yaml\") as item_file:\n",
    "    items = yaml.load(item_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-02-18 11:33:03,886 [info] function spec saved to path: bert_sentiment_analysis_serving.yaml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlrun.serving.states.TaskState at 0x7f895fb1f110>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = code_to_function(\n",
    "    name=items[\"name\"],\n",
    "    filename=items[\"spec\"][\"filename\"],\n",
    "    kind=items[\"spec\"][\"kind\"],\n",
    "    description=items[\"description\"],\n",
    "    categories=items[\"categories\"],\n",
    "    labels=items[\"labels\"],\n",
    "    image=items[\"spec\"][\"image\"],\n",
    "    requirements=items[\"spec\"][\"requirements\"],\n",
    ")\n",
    "\n",
    "fn.spec.default_class = \"SentimentClassifierServing\"\n",
    "fn.spec.max_replicas = 1\n",
    "fn.spec.readiness_timeout = 500\n",
    "\n",
    "fn.export(\"bert_sentiment_analysis_serving.yaml\")\n",
    "\n",
    "fn.add_model(\n",
    "    \"bert_classifier_v1\",\n",
    "    \"/User/demo_stocks/artifacts/models/bert_sentiment_analysis_model.pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"V3IO_HOME\" in list(os.environ):\n",
    "    from mlrun import mount_v3io\n",
    "\n",
    "    fn.apply(mount_v3io())\n",
    "else:\n",
    "    # is you set up mlrun using the instructions at\n",
    "    # https://github.com/mlrun/mlrun/blob/master/hack/local/README.md\n",
    "    from mlrun.platforms import mount_pvc\n",
    "\n",
    "    fn.apply(mount_pvc(\"nfsvol\", \"nfsvol\", \"/home/joyan/data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> 2021-02-18 11:33:03,931 [info] Starting remote function deploy\n",
      "2021-02-18 11:33:04  (info) Deploying function\n",
      "{'level': 'info', 'message': 'Deploying function', 'name': 'nlp-servers-sentiment-analysis-serving', 'time': 1613647984110.763}\n",
      "2021-02-18 11:33:04  (info) Building\n",
      "{'level': 'info', 'message': 'Building', 'name': 'nlp-servers-sentiment-analysis-serving', 'time': 1613647984110.7979, 'versionInfo': 'Label: 1.5.16, Git commit: ae43a6a560c2bec42d7ccfdf6e8e11a1e3cc3774, OS: linux, Arch: amd64, Go version: go1.14.3'}\n",
      "2021-02-18 11:33:04  (info) Staging files and preparing base images\n",
      "{'level': 'info', 'message': 'Staging files and preparing base images', 'name': 'deployer', 'time': 1613647984168.4219}\n",
      "2021-02-18 11:33:04  (info) Building processor image\n",
      "{'imageName': 'nuclio/nlp-servers-nlp-servers-sentiment-analysis-serving-processor:latest', 'level': 'info', 'message': 'Building processor image', 'name': 'deployer', 'time': 1613647984169.1504}\n",
      "2021-02-18 11:33:05  (info) Build complete\n",
      "{'level': 'info', 'message': 'Build complete', 'name': 'deployer', 'result': {'Image': 'nuclio/nlp-servers-nlp-servers-sentiment-analysis-serving-processor:latest', 'UpdatedFunctionConfig': {'metadata': {'annotations': {'nuclio.io/generated_by': 'function generated from https://github.com/AlonMaor14/functions.git#f08a11fb02d3aa75dd3a3b56b0bfb60be5dd1383:bert_sentiment_analysis_serving.py'}, 'labels': {'mlrun/class': 'serving', 'nuclio.io/project-name': 'nlp-servers'}, 'name': 'nlp-servers-sentiment-analysis-serving', 'namespace': 'default-tenant'}, 'spec': {'build': {'baseImage': 'mlrun/ml-models-gpu:0.6.0-rc13', 'codeEntryType': 'sourceCode', 'commands': ['python -m pip install transformers==3.0.2'], 'functionSourceCode': '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', 'noBaseImagesPull': True, 'offline': True, 'registry': 'docker-registry.default-tenant.app.vmdev36.lab.iguazeng.com:80'}, 'env': [{'name': 'V3IO_API', 'value': 'v3io-webapi.default-tenant.svc:8081'}, {'name': 'V3IO_USERNAME', 'value': 'admin'}, {'name': 'V3IO_ACCESS_KEY', 'value': '57638392-3001-4968-9b32-9c04bf5d826e'}, {'name': 'MLRUN_LOG_LEVEL', 'value': 'DEBUG'}, {'name': 'MLRUN_DBPATH', 'value': 'http://mlrun-api:8080'}, {'name': 'MLRUN_NAMESPACE', 'value': 'default-tenant'}, {'name': 'SERVING_SPEC_ENV', 'value': '{\"function_uri\": \"nlp-servers/sentiment-analysis-serving\", \"version\": \"v2\", \"parameters\": {}, \"graph\": {\"kind\": \"router\", \"routes\": {\"bert_classifier_v1\": {\"kind\": \"task\", \"class_name\": \"SentimentClassifierServing\", \"class_args\": {\"model_path\": \"/User/demo_stocks/artifacts/models/bert_sentiment_analysis_model.pt\"}}}}, \"load_mode\": null, \"functions\": {}, \"graph_initializer\": null, \"error_stream\": null, \"track_models\": null}'}], 'eventTimeout': '', 'handler': 'bert_sentiment_analysis_serving:handler', 'maxReplicas': 1, 'minReplicas': 1, 'platform': {}, 'readinessTimeoutSeconds': 500, 'resources': {}, 'runtime': 'python:3.6', 'securityContext': {}, 'serviceType': 'NodePort', 'triggers': {'default-http': {'attributes': {'serviceType': 'NodePort'}, 'class': '', 'kind': 'http', 'maxWorkers': 1, 'name': 'default-http'}}, 'volumes': [{'volume': {'flexVolume': {'driver': 'v3io/fuse', 'options': {'accessKey': '57638392-3001-4968-9b32-9c04bf5d826e'}}, 'name': 'v3io'}, 'volumeMount': {'mountPath': '/v3io', 'name': 'v3io'}}, {'volume': {'flexVolume': {'driver': 'v3io/fuse', 'options': {'accessKey': '57638392-3001-4968-9b32-9c04bf5d826e'}}, 'name': 'v3io'}, 'volumeMount': {'mountPath': '/User', 'name': 'v3io', 'subPath': 'users/admin'}}]}}}, 'time': 1613647985945.8706}\n",
      "Failed to deploy. Details:\n",
      "Downloading: 100%|██████████| 213k/213k [00:00<00:00, 492kB/s]  \n",
      "Exception raised while running init_context [worker_id=\"0\"]\n",
      "Caught unhandled exception while initializing [traceback=\"Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/states.py\", line 330, in init_object\n",
      "    self._object = self._class_object(**class_args)\n",
      "TypeError: __init__() got an unexpected keyword argument 'model_path'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/nuclio/_nuclio_wrapper.py\", line 350, in run_wrapper\n",
      "    args.trigger_name)\n",
      "  File \"/opt/nuclio/_nuclio_wrapper.py\", line 80, in __init__\n",
      "    getattr(entrypoint_module, 'init_context')(self._context)\n",
      "  File \"/opt/nuclio/bert_sentiment_analysis_serving.py\", line 51, in init_context\n",
      "    nuclio_init_hook(context, globals(), 'serving_v2')\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/mlrun/runtimes/nuclio.py\", line 31, in nuclio_init_hook\n",
      "    v2_serving_init(context, data)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/server.py\", line 206, in v2_serving_init\n",
      "    serving_handler = server.init(context, namespace or get_caller_globals())\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/server.py\", line 151, in init\n",
      "    self.graph.init_object(context, namespace, self.load_mode, reset=True)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/states.py\", line 481, in init_object\n",
      "    route.init_object(context, namespace, mode, reset=reset)\n",
      "  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/states.py\", line 333, in init_object\n",
      "    f\"failed to init state {self.name}, {e}\n",
      " args={self.class_args}\"\n",
      "TypeError: failed to init state bert_classifier_v1, __init__() got an unexpected keyword argument 'model_path'\n",
      " args={'model_path': '/User/demo_stocks/artifacts/models/bert_sentiment_analysis_model.pt'}\n",
      "\" || worker_id=\"0\" || err=\"failed to init state bert_classifier_v1, __init__() got an unexpected keyword argument 'model_path'\n",
      " args={'model_path': '/User/demo_stocks/artifacts/models/bert_sentiment_analysis_model.pt'}\"]\n",
      "> 2021-02-18 11:33:30,243 [error] Nuclio function failed to deploy\n"
     ]
    },
    {
     "ename": "RunError",
     "evalue": "cannot deploy Failed to deploy. Details:\n\rDownloading:   0%|          | 0.00/213k [00:00<?, ?B/s]\rDownloading:   1%|          | 2.05k/213k [00:00<00:14, 14.4kB/s]\rDownloading:  25%|██▌       | 54.3k/213k [00:00<00:07, 20.3kB/s]\rDownloading:  58%|█████▊    | 124k/213k [00:00<00:03, 28.4kB/s] \rDownloading: 100%|██████████| 213k/213k [00:00<00:00, 492kB/s] \nException raised while running init_context [worker_id=\"0\"]\nCaught unhandled exception while initializing [traceback=\"Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/states.py\", line 330, in init_object\n    self._object = self._class_object(**class_args)\nTypeError: __init__() got an unexpected keyword argument 'model_path'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/nuclio/_nuclio_wrapper.py\", line 350, in run_wrapper\n    args.trigger_name)\n  File \"/opt/nuclio/_nuclio_wrapper.py\", line 80, in __init__\n    getattr(entrypoint_module, 'init_context')(self._context)\n  File \"/opt/nuclio/bert_sentiment_analysis_serving.py\", line 51, in init_context\n    nuclio_init_hook(context, globals(), 'serving_v2')\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/runtimes/nuclio.py\", line 31, in nuclio_init_hook\n    v2_serving_init(context, data)\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/server.py\", line 206, in v2_serving_init\n    serving_handler = server.init(context, namespace or get_caller_globals())\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/server.py\", line 151, in init\n    self.graph.init_object(context, namespace, self.load_mode, reset=True)\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/states.py\", line 481, in init_object\n    route.init_object(context, namespace, mode, reset=reset)\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/states.py\", line 333, in init_object\n    f\"failed to init state {self.name}, {e}\n args={self.class_args}\"\nTypeError: failed to init state bert_classifier_v1, __init__() got an unexpected keyword argument 'model_path'\n args={'model_path': '/User/demo_stocks/artifacts/models/bert_sentiment_analysis_model.pt'}\n\" || worker_id=\"0\" || err=\"failed to init state bert_classifier_v1, __init__() got an unexpected keyword argument 'model_path'\n args={'model_path': '/User/demo_stocks/artifacts/models/bert_sentiment_analysis_model.pt'}\"]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRunError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-b2d259bb2d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maddr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nlp-servers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pythonlibs/jupyter/lib/python3.7/site-packages/mlrun/runtimes/serving.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, dashboard, project, tag, verbose)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_deploy_function_refs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"deploy root function {self.metadata.name} ...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeploy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdashboard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_runtime_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pythonlibs/jupyter/lib/python3.7/site-packages/mlrun/runtimes/function.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, dashboard, project, tag, verbose)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ready\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Nuclio function failed to deploy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRunError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"cannot deploy {text}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRunError\u001b[0m: cannot deploy Failed to deploy. Details:\n\rDownloading:   0%|          | 0.00/213k [00:00<?, ?B/s]\rDownloading:   1%|          | 2.05k/213k [00:00<00:14, 14.4kB/s]\rDownloading:  25%|██▌       | 54.3k/213k [00:00<00:07, 20.3kB/s]\rDownloading:  58%|█████▊    | 124k/213k [00:00<00:03, 28.4kB/s] \rDownloading: 100%|██████████| 213k/213k [00:00<00:00, 492kB/s] \nException raised while running init_context [worker_id=\"0\"]\nCaught unhandled exception while initializing [traceback=\"Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/states.py\", line 330, in init_object\n    self._object = self._class_object(**class_args)\nTypeError: __init__() got an unexpected keyword argument 'model_path'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/nuclio/_nuclio_wrapper.py\", line 350, in run_wrapper\n    args.trigger_name)\n  File \"/opt/nuclio/_nuclio_wrapper.py\", line 80, in __init__\n    getattr(entrypoint_module, 'init_context')(self._context)\n  File \"/opt/nuclio/bert_sentiment_analysis_serving.py\", line 51, in init_context\n    nuclio_init_hook(context, globals(), 'serving_v2')\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/runtimes/nuclio.py\", line 31, in nuclio_init_hook\n    v2_serving_init(context, data)\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/server.py\", line 206, in v2_serving_init\n    serving_handler = server.init(context, namespace or get_caller_globals())\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/server.py\", line 151, in init\n    self.graph.init_object(context, namespace, self.load_mode, reset=True)\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/states.py\", line 481, in init_object\n    route.init_object(context, namespace, mode, reset=reset)\n  File \"/opt/conda/lib/python3.7/site-packages/mlrun/serving/states.py\", line 333, in init_object\n    f\"failed to init state {self.name}, {e}\n args={self.class_args}\"\nTypeError: failed to init state bert_classifier_v1, __init__() got an unexpected keyword argument 'model_path'\n args={'model_path': '/User/demo_stocks/artifacts/models/bert_sentiment_analysis_model.pt'}\n\" || worker_id=\"0\" || err=\"failed to init state bert_classifier_v1, __init__() got an unexpected keyword argument 'model_path'\n args={'model_path': '/User/demo_stocks/artifacts/models/bert_sentiment_analysis_model.pt'}\"]"
     ]
    }
   ],
   "source": [
    "addr = fn.deploy(project='nlp-servers', verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remote example\n",
    "We will send a sentence to the model server via HTTP request. Note that the url below uses model server notation that directs our event to the predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "event_data = {'instances': ['I had a somewhat ok experience buying at that store.']}\n",
    "\n",
    "resp = requests.put(addr + '/bert_classifier_v1/predict', json=json.dumps(event_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model server classified the sentence as neutral. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
